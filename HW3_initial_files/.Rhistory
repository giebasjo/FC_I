knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
treg = function( pars, x, y, nu ){
neglogliketreg = function( pars, x, y, nu ){
resids = y - (x*pars[2] + pars[1])
return ((-1)*sum(dt(resids/pars[3], nu, log=TRUE)))
}
holdlm = lm( y ~ x )
parsinit = c(holdlm$coef[1],holdlm$coef[2], summary(holdlm)$sigma)
names(parsinit) = c("Beta_0", "Beta_1", "Sigma")
optout = optim(parsinit, neglogliketreg, x=x, y=y, nu=nu, hessian=T)
ret_Var = list( mle = optout$par, hessian=optout$hessian)
return (ret_Var)
}
## Testing on some equities using quantmod
daily_percentChange_O2C = OpCl(getSymbols("GS"))
library(quantmod)
_Gs = getSymbols("GS")
GS_ = getSymbols("GS")
GS_
head(GS_)
OpCl(GS_)
symb = getSymbols("AAPL", from = "2016-01-01")
symb
head(symb)
typeof(symb)
startDate = as.Date("2008-01-13") #Specify period of time we are interested in
endDate = as.Date("2012-01-12")
tickers <- c("AAPL","GS") #Define the tickers we are interested in
#Download the stock history (for all tickers)
symbs = getSymbols(tickers, env = stockData, src = "yahoo", from = startDate, to = endDate)
startDate = as.Date("2008-01-13") #Specify period of time we are interested in
endDate = as.Date("2012-01-12")
tickers <- c("AAPL","GS") #Define the tickers we are interested in
#Download the stock history (for all tickers)
symbs = getSymbols(tickers, src = "yahoo", from = startDate, to = endDate)
head(symbs)
endDate = as.Date("2012-01-12")
tickers <- c("AAPL","GS") #Define the tickers we are interested in
#Download the stock history (for all tickers)
symbs = getSymbols(tickers, src = "yahoo", from = startDate, to = endDate)
symbs
Cl(stockData$ARM)
library(quantmod)
startDate = as.Date("2008-01-13") #Specify period of time we are interested in
endDate = as.Date("2012-01-12")
tickers <- c("AAPL","GS") #Define the tickers we are interested in
#Download the stock history (for all tickers)
symbs = getSymbols(tickers, src = "yahoo", from = startDate, to = endDate)
head(symbs)
treg = function( pars, x, y, nu ){
# Set up the function to be minimized
neglogliketreg = function( pars, x, y, nu ){
resids = y - (x*pars[2] + pars[1])
return ((-1)*sum(dt(resids/pars[3], nu, log=TRUE)) + length(x)*log(pars[3]))
}
# Find init values through least squares
holdlm = lm( y ~ x )
parsinit = c(holdlm$coef[1],holdlm$coef[2], summary(holdlm)$sigma)
names(parsinit) = c("Beta_0", "Beta_1", "Sigma")
optout = optim(parsinit, neglogliketreg, x=x, y=y, nu=nu, hessian=T)
ret_Var = list( mle = optout$par, hessian=optout$hessian)
return (ret_Var)
}
nu = 5
sample_Vec = rt(1000, df=nu)
sample_Vec
clear
?df
?dt
clear
?optim
rm(list=ls())
x = rt(1000, df=5)
x
t_dist = function(sigma, x, nu){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.1
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, hessian=TRUE)
return (list(mle=optimout$par, hessian=optimout$hessian))
}
t_dist(sigma, x, nu = 10)
?optim
t_dist(sigma, x, nu = 10)
t_dist(sigma, x, nu = 10)
t_dist = function(sigma, x, nu){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.1
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, hessian=TRUE, method="Brent")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
t_dist(sigma, x, nu = 5)
rm(list=ls())
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(sigma, x, nu){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.1
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rt(1000, nu)
# test function
t_dist(sigma, x, nu)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.1
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(sigma, x, nu)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.1
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.1
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 20
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="L-BFGS-B")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 50
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="L-BFGS-B")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 25
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="L-BFGS-B")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 5
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="L-BFGS-B")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 1.5
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="L-BFGS-B")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.5
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.5
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
library(quantmod)
# Input : Log returns vector (lr_vec), d.o.f. (nu)
# Output: MLE for (sigma) with Standard Error (std_err)
# Using slides 30-34 of Lecture 4 Notes
# sigma is the parameter to be optimized
# x is a vector of log returns
# nu is the assumed d.o.f.
t_dist = function(x){
# Need the negative of the log-likelihood function.
# To be minimized using optim.
negllh = function(sigma, x, nu){
return ( (-1)*sum(sigma*dt(x,nu,log=TRUE)) )
}
# Define the starting point for the iterative optim method
sigma_startingEstimate = 0.5
# Perform the optimization
optimout = optim(sigma_startingEstimate,negllh, x=x, nu=5, hessian=TRUE, method="BFGS")
return (list(mle=optimout$par, hessian=optimout$hessian))
}
# simulate on a random data set/define nu (dof)
nu = 5
x = rnorm(1000, nu)
# test function
t_dist(x)
